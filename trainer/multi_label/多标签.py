import pandas as pd
import torch
from torch.utils.data import Dataset, DataLoader
from transformers import BertTokenizer, BertModel
from torch.optim import AdamW
import torch.nn as nn
from sklearn.model_selection import train_test_split
from sklearn.metrics import classification_report, f1_score
from tqdm import tqdm
import os

# ========= é…ç½® =========
MODEL_NAME = "hfl/chinese-bert-wwm-ext"   # ä¸­æ–‡ BERT
MAX_LEN = 128
BATCH_SIZE = 16
EPOCHS = 3
LR = 2e-5

# ========= 1. å®šä¹‰æ ‡ç­¾ä½“ç³» =========

parent_labels = ["èº«ä»½ä¿¡æ¯", "æ±½è½¦ä¿¡æ¯"]
child_labels = {
    "èº«ä»½ä¿¡æ¯": ["èº«ä»½è¯", "æ‰‹æœºå·", "å®¶åº­åœ°å€"],
    "æ±½è½¦ä¿¡æ¯": ["è½¦ç‰Œå·", "è½¦æ¶å·", "å‘åŠ¨æœºå‹å·"]
}

# æ˜ å°„åˆ° ID
parent2id = {l: i for i, l in enumerate(parent_labels)}
id2parent = {i: l for l, i in parent2id.items()}

child2id = {}
id2child = {}
for p, subs in child_labels.items():
    for s in subs:
        cid = len(child2id)
        child2id[s] = cid
        id2child[cid] = s

# ========= 2. æ•°æ®é›† =========
class HierDataset(Dataset):
    def __init__(self, df, tokenizer, max_len):
        self.texts = df["text"].tolist()
        self.labels = df["labels"].tolist()
        self.tokenizer = tokenizer
        self.max_len = max_len

    def __len__(self):
        return len(self.texts)

    def __getitem__(self, idx):
        text = self.texts[idx]
        label_list = self.labels[idx].split(",")

        # çˆ¶æ ‡ç­¾ one-hot
        parent_targets = torch.zeros(len(parent2id))
        for l in label_list:
            if l in parent2id:
                parent_targets[parent2id[l]] = 1

        # å­æ ‡ç­¾ one-hot
        child_targets = torch.zeros(len(child2id))
        for l in label_list:
            if l in child2id:
                child_targets[child2id[l]] = 1

        encodings = tokenizer(
            text,
            truncation=True,
            padding="max_length",
            max_length=self.max_len,
            return_tensors="pt"
        )

        item = {key: val.squeeze(0) for key, val in encodings.items()}
        item["parent_labels"] = parent_targets
        item["child_labels"] = child_targets
        return item

# åŠ è½½æ¨¡å‹ç±»ï¼ˆè¦å’Œè®­ç»ƒæ—¶ä¿æŒä¸€è‡´ï¼‰



# ========= 3. æ¨¡å‹ =========
class HierBERT(nn.Module):
    def __init__(self, model_name, num_parent, num_child):
        super(HierBERT, self).__init__()
        self.bert = BertModel.from_pretrained(model_name)
        hidden_size = self.bert.config.hidden_size
        self.parent_classifier = nn.Linear(hidden_size, num_parent)
        self.child_classifier = nn.Linear(hidden_size + num_parent, num_child)  # æ‹¼æ¥çˆ¶æ ‡ç­¾é¢„æµ‹ä½œä¸ºå­æ ‡ç­¾è¾“å…¥

    def forward(self, input_ids, attention_mask, parent_labels=None, child_labels=None):
        outputs = self.bert(input_ids=input_ids, attention_mask=attention_mask)
        pooled = outputs.pooler_output  # [batch, hidden]  # [CLS] å‘é‡
        parent_logits = self.parent_classifier(pooled)  # [batch, num_parent]
        # å°†çˆ¶é¢„æµ‹æ‹¼æ¥è¿›å­åˆ†ç±»å™¨
        parent_probs = torch.sigmoid(parent_logits)
        combined = torch.cat([pooled, parent_probs], dim=1)
        child_logits = self.child_classifier(combined)  # [batch, num_child]
        loss = None
        if parent_labels is not None and child_labels is not None:
            bce = nn.BCEWithLogitsLoss()
            parent_loss = bce(parent_logits, parent_labels)
            child_loss = bce(child_logits, child_labels)
            loss = parent_loss + child_loss
        return parent_logits, child_logits, loss  # æ³¨æ„ï¼šè¿”å› logitsï¼ˆæœª sigmoidï¼‰

# ========= 4. è®­ç»ƒå‡†å¤‡ =========
df = pd.read_csv("/test/data.csv")
train_df, val_df = train_test_split(df, test_size=0.2, random_state=42)

tokenizer = BertTokenizer.from_pretrained(MODEL_NAME)
train_dataset = HierDataset(train_df, tokenizer, MAX_LEN)
val_dataset = HierDataset(val_df, tokenizer, MAX_LEN)
train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)
val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE)

model = HierBERT(MODEL_NAME, len(parent2id), len(child2id))
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
model.to(device)
optimizer = AdamW(model.parameters(), lr=LR)

# ========= 5. è®­ç»ƒ & éªŒè¯ =========
from sklearn.metrics import f1_score

def train_epoch(model, dataloader, optimizer):
    model.train()
    total_loss = 0
    for batch in tqdm(dataloader, desc="Training"):
        input_ids = batch["input_ids"].to(device)
        attention_mask = batch["attention_mask"].to(device)
        parent_labels = batch["parent_labels"].to(device)
        child_labels = batch["child_labels"].to(device)

        # parent_logits, child_logits, loss = model.forward(input_ids, attention_mask, parent_labels, child_labels)
        # forward è¦ tensor
        parent_logits, child_logits, loss = model(input_ids, attention_mask, parent_labels, child_labels)

        optimizer.zero_grad()
        loss.backward()
        optimizer.step()
        total_loss += loss.item()
    return total_loss / len(dataloader)

def eval_model(model, dataloader):
    model.eval()
    all_parent_preds, all_parent_trues = [], []
    all_child_preds, all_child_trues = [], []
    with torch.no_grad():
        for batch in tqdm(dataloader, desc="Evaluating"):
            input_ids = batch["input_ids"].to(device)
            attention_mask = batch["attention_mask"].to(device)
            parent_labels = batch["parent_labels"].to(device)
            child_labels = batch["child_labels"].to(device)

            # parent_logits, child_logits, loss = model.forward(input_ids, attention_mask, parent_labels, child_labels)
            # forward è¦ tensor
            parent_logits, child_logits, loss = model(input_ids, attention_mask, parent_labels, child_labels)

            # logits â†’ prob â†’ numpy
            parent_probs = torch.sigmoid(parent_logits).cpu().numpy()
            child_probs = torch.sigmoid(child_logits).cpu().numpy()

            # label ä¹Ÿè½¬æˆ numpy ç”¨æ¥ç®— f1
            parent_labels = parent_labels.cpu().numpy()
            child_labels = child_labels.cpu().numpy()

            parent_preds = (parent_probs > 0.5).astype(int)
            child_preds = (child_probs > 0.5).astype(int)

            all_parent_preds.extend(parent_preds)
            all_parent_trues.extend(parent_labels)
            all_child_preds.extend(child_preds)
            all_child_trues.extend(child_labels)

    # micro-F1ï¼ˆæ€»ä½“ï¼‰
    parent_f1 = f1_score(all_parent_trues, all_parent_preds, average="micro", zero_division=0)
    child_f1 = f1_score(all_child_trues, all_child_preds, average="micro", zero_division=0)

    print("\nParent Report:")
    print(classification_report(all_parent_trues, all_parent_preds,
                                target_names=list(parent2id.keys()), zero_division=0))

    print("\nChild Report:")
    print(classification_report(all_child_trues, all_child_preds,
                                target_names=list(child2id.keys()), zero_division=0))

    return parent_f1, child_f1


def train1():
    # ========= 6. ä¸»å¾ªç¯ï¼ˆæ–°å¢ä¿å­˜é€»è¾‘ï¼‰ =========
    best_f1 = 0.0
    save_dir = "/saved_models"
    os.makedirs(save_dir, exist_ok=True)

    for epoch in range(EPOCHS):
        print(f"\n===== Epoch {epoch + 1}/{EPOCHS} =====")
        train_loss = train_epoch(model, train_loader, optimizer)
        print(f"Train Loss: {train_loss:.4f}")

        parent_f1, child_f1 = eval_model(model, val_loader)
        avg_f1 = (parent_f1 + child_f1) / 2
        print(f"Validation F1 - Parent: {parent_f1:.4f}, Child: {child_f1:.4f}, Avg: {avg_f1:.4f}")

        # ä¿å­˜å½“å‰ epoch æ¨¡å‹
        epoch_path = os.path.join(save_dir, f"hierbert_epoch{epoch + 1}.pt")
        torch.save(model.state_dict(), epoch_path)
        print(f"âœ… æ¨¡å‹å·²ä¿å­˜åˆ° {epoch_path}")

        # ä¿å­˜æœ€ä½³æ¨¡å‹
        if avg_f1 > best_f1:
            best_f1 = avg_f1
            best_path = os.path.join(save_dir, "hierbert_best.pt")
            torch.save(model.state_dict(), best_path)
            print(f"ğŸŒŸ æœ€ä½³æ¨¡å‹æ›´æ–°ï¼Œå·²ä¿å­˜åˆ° {best_path}")


# --- åŠ è½½æ¨¡å‹æƒé‡ï¼ˆæ›´ç¨³å¥ï¼‰ ---
def load_model(model_path):
    model = HierBERT(MODEL_NAME, num_parent=len(parent2id), num_child=len(child2id))

    # å…ˆåŠ è½½åˆ° CPUï¼Œä¹‹åå†ç§»åŠ¨åˆ° deviceï¼ˆå…¼å®¹ CPU/GPUï¼‰
    state = torch.load(model_path, map_location="cpu")

    # å¦‚æœä¿å­˜çš„æ˜¯ dictï¼ˆæœ€å¸¸è§ï¼‰ï¼Œè¿›ä¸€æ­¥å¤„ç†
    if isinstance(state, dict):
        # æœ‰äº›ä¿å­˜ä¼šåŒ…ä¸€å±‚ï¼Œå¦‚ {'state_dict': {...}}
        if "state_dict" in state and isinstance(state["state_dict"], dict):
            state = state["state_dict"]

        # å¤„ç† DataParallel ä¿å­˜æ—¶çš„ module. å‰ç¼€
        new_state = {}
        for k, v in state.items():
            new_key = k.replace("module.", "") if k.startswith("module.") else k
            new_state[new_key] = v
        state = new_state

        # å°è¯•ä¸¥æ ¼åŒ¹é…åŠ è½½ï¼›è‹¥ä¸åŒ¹é…å†é™çº§ä¸º strict=False å¹¶æ‰“å°è­¦å‘Š
        try:
            model.load_state_dict(state, strict=True)
        except RuntimeError as e:
            print(f"[WARN] ä¸¥æ ¼åŠ è½½å¤±è´¥ï¼š{e}\nå°è¯•ä½¿ç”¨ strict=False åŠ è½½ï¼ˆå¯èƒ½æœ‰éƒ¨åˆ†é”®ä¸åŒ¹é…ï¼‰ã€‚")
            model.load_state_dict(state, strict=False)
    else:
        # å¦‚æœä¿å­˜çš„æ˜¯æ•´ä¸ª model å¯¹è±¡ï¼ˆä¸å¸¸è§ï¼‰ï¼Œç›´æ¥æ›¿æ¢
        try:
            model = state
        except Exception as e:
            raise RuntimeError("æ— æ³•åŠ è½½æ¨¡å‹æƒé‡ï¼š%s" % e)

    model.to(device)
    model.eval()
    return model


# --- è§£æ model outputs çš„å·¥å…·å‡½æ•°ï¼ˆæ›´é²æ£’ï¼‰ ---
def parse_model_output(outputs):
    """
    å°è¯•ä» model(...) çš„è¿”å›å€¼ä¸­æå– parent_logits, child_logitsï¼ˆå‡ä¸ºæœª sigmoid çš„ logits tensorï¼‰ã€‚
    æ”¯æŒå¤šç§å½¢å¼çš„ outputsï¼šdict / tuple / list / åŒ…å« None çš„ tuple ç­‰ã€‚
    """
    parent_logits = None
    child_logits = None

    # 1) dict æƒ…å†µï¼ˆä¼˜å…ˆå¤„ç†ï¼‰
    if isinstance(outputs, dict):
        if "parent_logits" in outputs and "child_logits" in outputs:
            parent_logits = outputs["parent_logits"]
            child_logits = outputs["child_logits"]
        elif "parent_probs" in outputs and "child_probs" in outputs:
            # å°†æ¦‚ç‡è½¬å› logitsï¼ˆå°å¿ƒæ•°å€¼ç¨³å®šæ€§ï¼‰
            p = outputs["parent_probs"].clamp(1e-6, 1 - 1e-6)
            c = outputs["child_probs"].clamp(1e-6, 1 - 1e-6)
            parent_logits = torch.log(p / (1 - p))
            child_logits = torch.log(c / (1 - c))
        elif "logits" in outputs:
            logits = outputs["logits"]
            if isinstance(logits, torch.Tensor) and logits.dim() == 2 and logits.size(1) == (len(parent2id) + len(child2id)):
                parent_logits = logits[:, :len(parent2id)]
                child_logits = logits[:, len(parent2id):]
        else:
            # æ‰«æå­—å…¸ä¸­çš„ tensorï¼ŒæŒ‰åˆ—æ•°åŒ¹é…
            for v in outputs.values():
                if isinstance(v, torch.Tensor) and v.dim() == 2:
                    if v.size(1) == len(parent2id) and parent_logits is None:
                        parent_logits = v
                    elif v.size(1) == len(child2id) and child_logits is None:
                        child_logits = v

    # 2) tuple/list æƒ…å†µ
    elif isinstance(outputs, (tuple, list)):
        # æå–æ‰€æœ‰ tensor é¡¹ï¼ˆå¿½ç•¥ None æˆ–æ ‡é‡ï¼‰
        tensors = [x for x in outputs if isinstance(x, torch.Tensor)]
        # é¦–å…ˆå°è¯•æŒ‰åˆ—æ•°ç›´æ¥åŒ¹é…ï¼ˆé€‚åˆæœ¬ä¾‹ï¼š(parent_logits, child_logits, None)ï¼‰
        for t in tensors:
            if t.dim() == 2:
                if t.size(1) == len(parent2id) and parent_logits is None:
                    parent_logits = t
                elif t.size(1) == len(child2id) and child_logits is None:
                    child_logits = t

        # å¦‚æœè¿˜æ²¡æ‰¾åˆ°ï¼Œå°è¯•åœ¨ tuples ä¸­å¯»æ‰¾å½¢çŠ¶æœ€æ¥è¿‘çš„ tensorï¼ˆæ›´å®½æ¾çš„å›é€€ï¼‰
        if parent_logits is None or child_logits is None:
            candidates = [t for t in tensors if t.dim() == 2]
            for t in candidates:
                if parent_logits is None and t.size(1) == len(parent2id):
                    parent_logits = t
                if child_logits is None and t.size(1) == len(child2id):
                    child_logits = t

    # 3) å¤±è´¥æ—¶æ‰“å° debug ä¿¡æ¯ï¼Œä¾¿äºå®šä½
    if parent_logits is None or child_logits is None:
        # æ‰“å°ä¸€äº›æœ‰ç”¨ä¿¡æ¯å¸®åŠ©è°ƒè¯•ï¼Œç„¶åæŠ›é”™
        print("=== DEBUG: æ— æ³•è§£æ model è¾“å‡ºï¼Œoutputs å†…å®¹å¦‚ä¸‹ï¼š ===")
        print("Type:", type(outputs))
        try:
            # å¯èƒ½æ˜¯è¾ƒå¤§çš„ tensorï¼Œæ‰“å° summary è€Œéå…¨éƒ¨å†…å®¹
            if isinstance(outputs, (tuple, list)):
                for i, o in enumerate(outputs):
                    print(f"  outputs[{i}]: type={type(o)}", end="")
                    if isinstance(o, torch.Tensor):
                        print(f", shape={tuple(o.shape)}, device={o.device}")
                    else:
                        print(f", value={o}")
            elif isinstance(outputs, dict):
                for k, v in outputs.items():
                    print(f"  outputs['{k}']: type={type(v)}", end="")
                    if isinstance(v, torch.Tensor):
                        print(f", shape={tuple(v.shape)}, device={v.device}")
                    else:
                        print(f", value={v}")
            else:
                print(outputs)
        except Exception:
            print("  (æ‰“å° outputs æ—¶å‘ç”Ÿé”™è¯¯)")
        raise ValueError(
            "æ— æ³•ä» model è¾“å‡ºä¸­æå– parent/child logitsã€‚è¯·æ ¹æ®ä¸Šæ–¹ debug ä¿¡æ¯è°ƒæ•´æ¨¡å‹çš„ forward è¿”å›å€¼æˆ–ä¿®æ”¹ parse_model_outputã€‚"
        )

    return parent_logits, child_logits

# --- é¢„æµ‹å‡½æ•°ï¼ˆè¿”å›æ ‡ç­¾ + æ¦‚ç‡ï¼‰ ---
def predict(text_or_texts, model, tokenizer, threshold=0.5):
    # æ”¯æŒå•æ¡æ–‡æœ¬æˆ–å¤šæ¡æ–‡æœ¬åˆ—è¡¨
    single = False
    if isinstance(text_or_texts, str):
        texts = [text_or_texts]
        single = True
    else:
        texts = list(text_or_texts)

    enc = tokenizer(texts, truncation=True, padding="max_length", max_length=MAX_LEN, return_tensors="pt")
    input_ids = enc["input_ids"].to(device)
    attention_mask = enc["attention_mask"].to(device)

    with torch.no_grad():
        outputs = model(input_ids=input_ids, attention_mask=attention_mask)

    parent_logits, child_logits = parse_model_output(outputs)
    parent_probs = torch.sigmoid(parent_logits).cpu().numpy()
    child_probs = torch.sigmoid(child_logits).cpu().numpy()

    results = []
    for i in range(parent_probs.shape[0]):
        p_probs = parent_probs[i]
        c_probs = child_probs[i]
        p_pred = [id2parent[idx] for idx, p in enumerate(p_probs) if p >= threshold]
        c_pred = [id2child[idx] for idx, p in enumerate(c_probs) if p >= threshold]
        results.append({
            "parent_pred": p_pred,
            "parent_probs": p_probs,
            "child_pred": c_pred,
            "child_probs": c_probs
        })

    return results[0] if single else results


def run(text: str):
    MODEL_PATH = "/saved_models/hierbert_best.pt"
    out = predict(text, load_model(MODEL_PATH), tokenizer, threshold=0.5)
    print("çˆ¶æ ‡ç­¾é¢„æµ‹ï¼š", out["parent_pred"])
    print("å­æ ‡ç­¾é¢„æµ‹ï¼š", out["child_pred"])
    print("çˆ¶æ ‡ç­¾æ¦‚ç‡ï¼š", out["parent_probs"])
    print("å­æ ‡ç­¾æ¦‚ç‡ï¼š", out["child_probs"])
    print(out)


if __name__ == "__main__":
    # train1()
    run("å‘åŠ¨æœºå‹å·ï¼šJLH-4G20TDJï¼›å®¶åº­ä½å€ï¼šåŒ—äº¬å¸‚æµ·æ·€åŒºXXå°åŒº")
    pass
